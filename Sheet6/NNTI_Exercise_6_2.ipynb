{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3e234d",
   "metadata": {},
   "source": [
    "Team member 1 (name, email, id): Mhd Jawad Al Rahwanji, mhal00002@stud.uni-saarland.de, 7038980\n",
    "Team member 2 (name, email, id): Christian Singer, chsi00002@stud.uni-saarland.de, 7039059\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56009306-ebf1-4707-a8fb-58bf315689b5",
   "metadata": {},
   "source": [
    "### Note: This assignment will extensively refer to coding exercise in assignment 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956dd05",
   "metadata": {},
   "source": [
    "## 6.2.a Building your own Neural-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada51e96",
   "metadata": {},
   "source": [
    "Import numpy, which is really all we need to create our own NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb034177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252678aa",
   "metadata": {},
   "source": [
    "Recall that our simple neural network consisted of two layers. We also added an `activation` function as a non-linearity to the output of our intermediate layer. Given an input $\\mathbf{x} \\in \\mathbb{R}^n $ we have\n",
    "\n",
    "$ \\mathbf{h} = f^{(1)}(\\mathbf{x}; \\mathbf{W},c) = activation\\_fn(\\mathbf{W}^\\mathsf{T} \\mathbf{x} + c) $ \n",
    "\n",
    "$ \\mathbf{y} = f^{(2)}(\\mathbf{h}; \\mathbf{w},b) = \\text{$ softmax $}( \\mathbf{w}^\\mathsf{T} \\mathbf{h} + b) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bd9f9",
   "metadata": {},
   "source": [
    "In this exercise you will create your own network and are free to implement it with your own design choices. However, we will do it in a way that allows you to specify the depth of network, i.e. we extend our network such that there isn't just one $\\mathbf{h}$ intermediate layers, but rather $n$ of them $\\mathbf{h}_{i}$ with $i \\in \\{1,..., n\\}$\n",
    "\n",
    "**NOTE**: You are not allowed to use any built-in functions to calculate Leaky_ReLU, Softmax or the forward/backward pass directly.\n",
    "\n",
    "**NOTE 2**: Remember to include the non-linearity at every layer. Remember to also add the bias to every layer. Finally, remember to apply the softmax in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66f378",
   "metadata": {},
   "source": [
    "## ToDo: Rewrite the Leaky_ReLu and Softmax function as Class and implement a function in each of them to calculate gradients (1 point)\n",
    "Remember that in PyTorch, these are implemented as classes so we also want to have them as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3114d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU():\n",
    "    \"\"\" Leaky ReLU activation function. \"\"\"\n",
    "    def __init__(self, alpha: float=0.01):\n",
    "        \"\"\" Initialize LeakyReLU activation function. \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.trainable = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass of LeakyReLU activation function. \"\"\"\n",
    "        self.input = x\n",
    "        return np.maximum(x, self.alpha * x)\n",
    "\n",
    "    # Make forward method callable like LeakyReLU(x)\n",
    "    __call__ = forward\n",
    "\n",
    "    def gradient(self, x):\n",
    "        \"\"\" Calculate gradient of function.\"\"\"\n",
    "        jacobian = np.where(x > 0, 1, self.alpha)\n",
    "        return jacobian\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\" Backward pass of LeakyReLU activation function. \"\"\"\n",
    "        return grad_output * self.gradient(self.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b77127",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    \"\"\" Softmax activation function. \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initialize Softmax activation function. \"\"\"\n",
    "        self.trainable = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass of Softmax activation function. \"\"\"\n",
    "        z = x - np.max(x)\n",
    "        exp = np.exp(z)\n",
    "        probs = exp / np.sum(exp)\n",
    "        return probs\n",
    "\n",
    "    # Make forward method callable lik Softmax(x)\n",
    "    __call__ = forward\n",
    "\n",
    "    def gradient(self, y):\n",
    "        \"\"\" Calculate gradient of function.\"\"\"\n",
    "        jacobian = np.diag(y) - np.outer(y, y)\n",
    "        return jacobian\n",
    "\n",
    "    def backward(self, y):\n",
    "        \"\"\" Backward pass of Softmax activation function. \"\"\"\n",
    "        jacobian = self.gradient(y)\n",
    "        return np.dot(jacobian,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dde97f",
   "metadata": {},
   "source": [
    "## ToDo: Calculate the gradient using your implemented functions in their respective classes and validate by manually calculating gradients using a toy value. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76203a80-9b13-4012-879f-10f78bc6dd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient LeakyRelu: [1. 1. 1.]\n",
      "Gradient Softmax: [[ 0.25   -0.125  -0.125 ]\n",
      " [-0.125   0.1875 -0.0625]\n",
      " [-0.125  -0.0625  0.1875]]\n"
     ]
    }
   ],
   "source": [
    "activation1 = LeakyReLU()\n",
    "activation2 = Softmax()\n",
    "\n",
    "data = np.array([1/2, 1/4, 1/4])\n",
    "\n",
    "print(f\"Gradient LeakyRelu: {activation1.gradient(data)}\")\n",
    "print(f\"Gradient Softmax: {activation2.gradient(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be4f42",
   "metadata": {},
   "source": [
    "## ToDo: Rewrite the code from Assignment 4 to include backpropagation in your class without using pytorch. Remember to use your Leaky_ReLu class here as activation function. (1.5 points)\n",
    "#### Feel free to refer to your solutions from Assignment 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e87fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer():\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\" Initialize LinearLayer with He-method. \"\"\"\n",
    "        self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2 / input_dim)\n",
    "        self.b = np.random.randn(output_dim) * np.sqrt(2 / input_dim)\n",
    "        self.trainable = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass of LinearLayer. \"\"\"\n",
    "        self.input = x\n",
    "        return np.dot(self.W, x) + self.b\n",
    "\n",
    "    # Make forward method callable like LinearLayer(x)\n",
    "    __call__ = forward\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\" Backward pass of LinearLayer. \"\"\"\n",
    "        x = self.input\n",
    "        self.grad_W = np.outer(grad_output, x)\n",
    "        self.grad_b = grad_output\n",
    "        return np.dot(self.W.T, grad_output)\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\" Update parameters of LinearLayer. \"\"\"\n",
    "        self.W -= learning_rate * self.grad_W\n",
    "        self.b -= learning_rate * self.grad_b\n",
    "\n",
    "\n",
    "class Sequential():\n",
    "    \"\"\" Sequential model. \"\"\"\n",
    "    def __init__(self, *layers):\n",
    "        \"\"\" Initialize Sequential model. \"\"\"\n",
    "        self.layers = list(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass of Sequential model. \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    # Make forward method callable like Sequential(x)\n",
    "    __call__ = forward\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\" Backward pass of Sequential model. \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "        return grad_output\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        \"\"\" Update parameters of Sequential model. \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if layer.trainable:\n",
    "                layer.update_params(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b823a72",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.2.b.2 Training a network for MNIST (1.5 points)\n",
    "\n",
    "Now that we know how to train a Neural network in Pytorch. Let's start training and evaluating our model using a very standard dataset, for now let's use MNIST. Design a network from scracth using PyTorch and include the followings. Remember that we need to use forward-propagation and backprop.\n",
    "- Training Loop\n",
    "- Optimization \n",
    "- Evaluating Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchFFNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A pytorch implementation to do classification for MNIST dataset.\n",
    "    \"\"\"\n",
    "    def __init__():\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f17a8-4877-4813-b4ad-cc2d41b05be9",
   "metadata": {},
   "source": [
    "### ToDo: Implement functions for Stochastic Gradient Descent and Stochastic Gradient Descent with momentum and plot the difference on how they change the value for gradients. ( 1 + 1 (Bonus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99bc82-b090-43a0-91a3-8b2e8352c772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
