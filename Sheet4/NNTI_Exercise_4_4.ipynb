{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3e234d",
   "metadata": {},
   "source": [
    "Team member 1 (name, email, id): Mhd Jawad Al Rahwanji, mhal00002@stud.uni-saarland.de, 7038980\n",
    "\n",
    "Team member 2 (name, email, id): Christian Singer, chsi00002@stud.uni-saarland.de, 7039059"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956dd05",
   "metadata": {},
   "source": [
    "## 4.4.a Building your own feed-forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada51e96",
   "metadata": {},
   "source": [
    "Import numpy, which is really all we need to create our own NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb034177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252678aa",
   "metadata": {},
   "source": [
    "Recall that our simple neural network consisted of two layers. We also added a `ReLU` function as a non-linearity to the output of our intermediate layer. Given an input $\\mathbf{x} \\in \\mathbb{R}^n $ we have\n",
    "\n",
    "$\\mathbf{h} = f^{(1)}(\\mathbf{x}; \\mathbf{W},c) = \\text{ ReLU }(\\mathbf{W}^\\mathsf{T} \\mathbf{x} + c)$\n",
    "\n",
    "$\\mathbf{y} = f^{(2)}(\\mathbf{h}; \\mathbf{w},b) = \\text{ softmax }( \\mathbf{w}^\\mathsf{T} \\mathbf{h} + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bd9f9",
   "metadata": {},
   "source": [
    "In this exercise you will create your own network. However, we will do it in a way that allows you to specify the depth of network, i.e. we extend our network such that there isn't just one $\\mathbf{h}$ intermediate layers, but rather $n$ of them $\\mathbf{h}_{i}$ with $i \\in \\{1,..., n\\}$\n",
    "\n",
    "**NOTE**: You are not allowed to use any built-in functions to calculate the ReLU, Softmax or the forward pass directly.\n",
    "\n",
    "**NOTE 2**: Remember to include the non-linearity at every layer. Remember to also add the bias to every layer. Finally, remember to apply the softmax in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3114d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Implement the ReLU function as defined in the lecture\n",
    "    Input: an array of numbers\n",
    "    Output: ReLU(x)\n",
    "    \"\"\"\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b77127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Implement the `softmax` function as defined in the lecture\n",
    "    \"\"\"\n",
    "    # Make softmax numerically stable.\n",
    "    z = x - np.max(x)\n",
    "    return np.exp(z)/np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e87fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNetwork:\n",
    "    \"\"\"\n",
    "    Class representing the feed-forward neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int,\n",
    "                 output_dim: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_dim: dimensionality of `x`\n",
    "        hidden_dim: dimensionality of the intermediate `h_i`\n",
    "        output_dim: dimensionality of `y`\n",
    "        hidden_size: number of intermediate layers `h_i`\n",
    "        \"\"\"\n",
    "        # First layer\n",
    "        self.W = [np.random.randn(hidden_dim, input_dim)]\n",
    "        self.b = [np.random.randn(hidden_dim)]\n",
    "        # Hidden layers\n",
    "        for i in range(hidden_size-2):\n",
    "            self.W.append(np.random.randn(hidden_dim, hidden_dim))\n",
    "            self.b.append(np.random.randn(hidden_dim))\n",
    "        # Last layer\n",
    "        self.W.append(np.random.randn(output_dim, hidden_dim))\n",
    "        self.b.append(np.random.randn(output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: input to the neural network\n",
    "        \n",
    "        Output:\n",
    "        `y`, i.e. the prediction of the network\n",
    "        \"\"\"\n",
    "        for i in range(len(self.W)-1):\n",
    "            x = relu(np.dot(self.W[i], x) + self.b[i])\n",
    "\n",
    "        return softmax(np.dot(self.W[i+1], x) + self.b[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd64a7",
   "metadata": {},
   "source": [
    "Your implementation needs to be compatible with the following test code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2109ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9518352  0.01096434 0.03720046]\n",
      "(3,)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# A configuration that reflects the example from the lecture\n",
    "# i.e. our input is of size 2, our intermediate layers are also of size 2,\n",
    "# and we will only have 1 hidden layer.\n",
    "network = FFNetwork(2, 2, 3, 1)\n",
    "out = network.forward([1.,0.])\n",
    "print(out)\n",
    "print(out.shape)\n",
    "print(out.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae8166",
   "metadata": {},
   "source": [
    "Disclaimer: Do not expect a correct output at this stage, you are simply building the structure of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d5d26",
   "metadata": {},
   "source": [
    "However, our setup also allows us to create larger networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "452ce6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.16295674e-11 1.00000000e+00]\n",
      "(2,)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "network = FFNetwork(2, 3, 2, 4)\n",
    "out = network.forward([1.,0.])\n",
    "print(out)\n",
    "print(out.shape)\n",
    "print(out.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc484337",
   "metadata": {},
   "source": [
    "Some sanity checks:\n",
    "\n",
    "1. You should be seeing the number of units you specified as output units in your output.\n",
    "1. The numbers in your outputs should be in the range $[0,1]$\n",
    "1. The numbers should add up to $1$\n",
    "1. Varying the structure of the network should not break its functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba70186",
   "metadata": {},
   "source": [
    "## 4.4.b Implementing a feed-forward network using `torch`\n",
    "\n",
    "### 4.4.b.1 Creating the network (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167133e",
   "metadata": {},
   "source": [
    "For this we will be using the `nn` module of `torch`, which contains modules representing types of layers. In your case, the specific relevant module would be that of a *fully connected linear layer*.\n",
    "\n",
    "We will also be using the `nn.functional` module to take advantage of the built in functions for ReLU and Softmax. In this exercise, you are allowed to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3298c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed85010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchFFNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A `torch` version of the network implemented for 4.3.b\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int,\n",
    "                 output_dim: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_dim: dimensionality of `x`\n",
    "        hidden_dim: dimensionality of the intermediate `h_i`\n",
    "        output_dim: dimensionality of `y`\n",
    "        hidden_size: number of intermediate layers `h_i`\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = [nn.Linear(input_dim, hidden_dim)]\n",
    "        # Hidden layers\n",
    "        for i in range(hidden_size-2):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        # Last layer\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" forward pass. \"\"\"\n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = F.relu(self.layers[i](x))\n",
    "\n",
    "        return F.softmax(self.layers[i+1](x), dim=-1)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c871745",
   "metadata": {},
   "source": [
    "Your implementation, once more, needs to be compatible with the following test code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c1c4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_network = TorchFFNetwork(2, 3, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2cfabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4670, 0.5330],\n",
      "        [0.4670, 0.5330]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = torch_network(torch.tensor([[1.,0.], [2.,2.]]))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addfc177",
   "metadata": {},
   "source": [
    "Note that the `forward` method is automatically called when you call your network object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b823a72",
   "metadata": {},
   "source": [
    "### 4.4.b.2 Training your network (1 point)\n",
    "\n",
    "Even though we have not covered how training actually works, we will proceed with the training of the a neural network as a blackbox procedure and we will later on learn the internals of the training process (and even implement them ourselves!).\n",
    "\n",
    "For now, train a neural network (the one you created above) to learn the XOR operation. You are to create a neural network with the appropriate number of input variables, an intermediate hidden layer with 2 units and an output layer with 2 units.\n",
    "\n",
    "Notes:\n",
    "- Please read [this introduction to the optimization loop in PyTorch](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#optimization-loop). It should give you a good overview to what PyTorch needs from you to train a neural network.\n",
    "- You are to train the network until the network learns the operation. Remember to set your random seeds so the results are reproducible.\n",
    "- There are many optimizers available and Adam is an optimizer that's more complex than SGD. It has not yet been covered in the lecture but its usage in code is equivalent to that of SGD and performs much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "399c98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our training X, where each instance includes an x1 and an x2, (where the operation is defined as x1 XOR x2)\n",
    "training_x = [[0,0], [0,1], [1,0], [1,1]]\n",
    "\n",
    "# We have only covered softmax in the lecture, so we format the output as follows:\n",
    "training_y = [[1,0], [0,1], [0,1], [1,0]]\n",
    "\n",
    "# The Y is formatted such that its first element corresponds to the probability of the XOR resulting in a 0\n",
    "# and the second element to the probability of the XOR resulting in a 1\n",
    "\n",
    "################################################################\n",
    "X = torch.tensor(training_x, dtype=torch.float)\n",
    "Y = torch.tensor(training_y, dtype=torch.float)\n",
    "\n",
    "data = (X,Y)\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object Module.parameters at 0x000001E5FB7D3AC0>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TorchFFNetwork(2,2,2,1)\n",
    "model.parameters()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8697ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model from the previous class and pick a learning rate\n",
    "torch.manual_seed(42)\n",
    "model = TorchFFNetwork(2,2,2,1)\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.01, momentum=0.9\n",
    ")\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65516831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data, model, loss_fn, optimizer, num_epochs=100):\n",
    "    for epoch in range(num_epochs+1):\n",
    "        out = model(data[0])\n",
    "        loss = loss_fn(out, data[1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (not epoch % 5) or epoch < 10:\n",
    "            print(f\"Loss in epoch {epoch}: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58208004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.2744489908218384\n",
      "Loss in epoch 1: 0.274308443069458\n",
      "Loss in epoch 2: 0.2740419805049896\n",
      "Loss in epoch 3: 0.27366337180137634\n",
      "Loss in epoch 4: 0.2731854021549225\n",
      "Loss in epoch 5: 0.27261993288993835\n",
      "Loss in epoch 6: 0.2719777524471283\n",
      "Loss in epoch 7: 0.27126896381378174\n",
      "Loss in epoch 8: 0.2705026865005493\n",
      "Loss in epoch 9: 0.2699359059333801\n",
      "Loss in epoch 10: 0.2693568766117096\n",
      "Loss in epoch 15: 0.26603254675865173\n",
      "Loss in epoch 20: 0.2625885605812073\n",
      "Loss in epoch 25: 0.2594967782497406\n",
      "Loss in epoch 30: 0.25693631172180176\n",
      "Loss in epoch 35: 0.25622570514678955\n",
      "Loss in epoch 40: 0.25579890608787537\n",
      "Loss in epoch 45: 0.2554698884487152\n",
      "Loss in epoch 50: 0.2551637589931488\n",
      "Loss in epoch 55: 0.2548638880252838\n",
      "Loss in epoch 60: 0.2545751631259918\n",
      "Loss in epoch 65: 0.2543051242828369\n",
      "Loss in epoch 70: 0.254057377576828\n",
      "Loss in epoch 75: 0.25383156538009644\n",
      "Loss in epoch 80: 0.2536250352859497\n",
      "Loss in epoch 85: 0.2534346878528595\n",
      "Loss in epoch 90: 0.2532578408718109\n",
      "Loss in epoch 95: 0.2530924081802368\n",
      "Loss in epoch 100: 0.2529372572898865\n"
     ]
    }
   ],
   "source": [
    "train_loop(data, model, loss_fn, optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
