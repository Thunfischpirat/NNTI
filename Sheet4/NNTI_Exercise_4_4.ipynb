{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3e234d",
   "metadata": {},
   "source": [
    "Team member 1 (name, email, id): Mhd Jawad Al Rahwanji, mhal00002@stud.uni-saarland.de, 7038980\n",
    "\n",
    "Team member 2 (name, email, id): Christian Singer, chsi00002@stud.uni-saarland.de, 7039059"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956dd05",
   "metadata": {},
   "source": [
    "## 4.4.a Building your own feed-forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada51e96",
   "metadata": {},
   "source": [
    "Import numpy, which is really all we need to create our own NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb034177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252678aa",
   "metadata": {},
   "source": [
    "Recall that our simple neural network consisted of two layers. We also added a `ReLU` function as a non-linearity to the output of our intermediate layer. Given an input $\\mathbf{x} \\in \\mathbb{R}^n $ we have\n",
    "\n",
    "$\\mathbf{h} = f^{(1)}(\\mathbf{x}; \\mathbf{W},c) = \\text{ ReLU }(\\mathbf{W}^\\mathsf{T} \\mathbf{x} + c)$\n",
    "\n",
    "$\\mathbf{y} = f^{(2)}(\\mathbf{h}; \\mathbf{w},b) = \\text{ softmax }( \\mathbf{w}^\\mathsf{T} \\mathbf{h} + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bd9f9",
   "metadata": {},
   "source": [
    "In this exercise you will create your own network. However, we will do it in a way that allows you to specify the depth of network, i.e. we extend our network such that there isn't just one $\\mathbf{h}$ intermediate layers, but rather $n$ of them $\\mathbf{h}_{i}$ with $i \\in \\{1,..., n\\}$\n",
    "\n",
    "**NOTE**: You are not allowed to use any built-in functions to calculate the ReLU, Softmax or the forward pass directly.\n",
    "\n",
    "**NOTE 2**: Remember to include the non-linearity at every layer. Remember to also add the bias to every layer. Finally, remember to apply the softmax in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3114d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Implement the ReLU function as defined in the lecture\n",
    "    Input: an array of numbers\n",
    "    Output: ReLU(x)\n",
    "    \"\"\"\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8b77127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Implement the `softmax` function as defined in the lecture\n",
    "    \"\"\"\n",
    "    # Make softmax numerically stable.\n",
    "    z = x - np.max(x)\n",
    "    return np.exp(z)/np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8e87fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNetwork:\n",
    "    \"\"\"\n",
    "    Class representing the feed-forward neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int,\n",
    "                 output_dim: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_dim: dimensionality of `x`\n",
    "        hidden_dim: dimensionality of the intermediate `h_i`\n",
    "        output_dim: dimensionality of `y`\n",
    "        hidden_size: number of intermediate layers `h_i`\n",
    "        \"\"\"\n",
    "        # First layer\n",
    "        self.W = [np.random.randn(hidden_dim, input_dim)]\n",
    "        self.b = [np.random.randn(hidden_dim, 1)]\n",
    "        # Hidden layers\n",
    "        for i in range(hidden_size-2):\n",
    "            self.W.append(np.random.randn(hidden_dim, hidden_dim))\n",
    "            self.b.append(np.random.randn(hidden_dim, 1))\n",
    "        # Last layer\n",
    "        self.W.append(np.random.randn(output_dim, hidden_dim))\n",
    "        self.b.append(np.random.randn(output_dim, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: input to the neural network\n",
    "        \n",
    "        Output:\n",
    "        `y`, i.e. the prediction of the network\n",
    "        \"\"\"\n",
    "        res = x\n",
    "\n",
    "        for i in range(len(self.W)-1):\n",
    "            res = relu(np.dot(self.W[i], res) + self.b[i])\n",
    "\n",
    "        return softmax(np.dot(self.W[i+1], res) + self.b[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd64a7",
   "metadata": {},
   "source": [
    "Your implementation needs to be compatible with the following test code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2109ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.56859336 0.30366054]\n",
      " [0.07154498 0.05620112]]\n",
      "(2, 2)\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# A configuration that reflects the example from the lecture\n",
    "# i.e. our input is of size 2, our intermediate layers are also of size 2,\n",
    "# and we will only have 1 hidden layer.\n",
    "network = FFNetwork(2, 2, 2, 1)\n",
    "out = network.forward([1.,0.])\n",
    "print(out)\n",
    "print(out.shape)\n",
    "print(out.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dae8166",
   "metadata": {},
   "source": [
    "Disclaimer: Do not expect a correct output at this stage, you are simply building the structure of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d5d26",
   "metadata": {},
   "source": [
    "However, our setup also allows us to create larger networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "452ce6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.95385696e-11 3.76113523e-09 7.45246071e-11]\n",
      " [4.13120877e-01 9.66912759e-03 5.77209992e-01]]\n",
      "(2, 3)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "network = FFNetwork(2, 3, 2, 4)\n",
    "out = network.forward([1.,0.])\n",
    "print(out)\n",
    "print(out.shape)\n",
    "print(out.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc484337",
   "metadata": {},
   "source": [
    "Some sanity checks:\n",
    "\n",
    "1. You should be seeing the number of units you specified as output units in your output.\n",
    "1. The numbers in your outputs should be in the range $[0,1]$\n",
    "1. The numbers should add up to $1$\n",
    "1. Varying the structure of the network should not break its functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba70186",
   "metadata": {},
   "source": [
    "## 4.4.b Implementing a feed-forward network using `torch`\n",
    "\n",
    "### 4.4.b.1 Creating the network (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167133e",
   "metadata": {},
   "source": [
    "For this we will be using the `nn` module of `torch`, which contains modules representing types of layers. In your case, the specific relevant module would be that of a *fully connected linear layer*.\n",
    "\n",
    "We will also be using the `nn.functional` module to take advantage of the built in functions for ReLU and Softmax. In this exercise, you are allowed to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3298c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed85010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchFFNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A `torch` version of the network implemented for 4.3.b\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int,\n",
    "                 output_dim: int, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input_dim: dimensionality of `x`\n",
    "        hidden_dim: dimensionality of the intermediate `h_i`\n",
    "        output_dim: dimensionality of `y`\n",
    "        hidden_size: number of intermediate layers `h_i`\n",
    "        \"\"\"\n",
    "        ## SOLUTION ##\n",
    "        pass\n",
    "        ## SOLUTION ##\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## SOLUTION ##\n",
    "        pass\n",
    "        ## SOLUTION ##\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c871745",
   "metadata": {},
   "source": [
    "Your implementation, once more, needs to be compatible with the following test code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1c4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_network = TorchFFNetwork(2, 3, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a2cfabc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TorchFFNetwork' object has no attribute '_backward_hooks'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [11], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m----> 2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mtorch_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1.\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m0.\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\Desktop\\UdS\\NNTI\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1188\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1185\u001B[0m forward_call \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_get_tracing_state() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward)\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m-> 1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backward_hooks\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m   1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\UdS\\NNTI\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1265\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1263\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[0;32m   1264\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m-> 1265\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m   1266\u001B[0m     \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'TorchFFNetwork' object has no attribute '_backward_hooks'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch_network(torch.tensor([1.,0.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addfc177",
   "metadata": {},
   "source": [
    "Note that the `forward` method is automatically called when you call your network object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b823a72",
   "metadata": {},
   "source": [
    "### 4.4.b.2 Training your network (1 point)\n",
    "\n",
    "Even though we have not covered how training actually works, we will proceed with the training of the a neural network as a blackbox procedure and we will later on learn the internals of the training process (and even implement them ourselves!).\n",
    "\n",
    "For now, train a neural network (the one you created above) to learn the XOR operation. You are to create a neural network with the appropriate number of input variables, an intermediate hidden layer with 2 units and an output layer with 2 units.\n",
    "\n",
    "Notes:\n",
    "- Please read [this introduction to the optimization loop in PyTorch](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#optimization-loop). It should give you a good overview to what PyTorch needs from you to train a neural network.\n",
    "- You are to train the network until the network learns the operation. Remember to set your random seeds so the results are reproducible.\n",
    "- There are many optimizers available and Adam is an optimizer that's more complex than SGD. It has not yet been covered in the lecture but its usage in code is equivalent to that of SGD and performs much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399c98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our training X, where each instance includes an x1 and an x2, (where the operation is defined as x1 XOR x2)\n",
    "training_x = [[0,0], [0,1], [1,0], [1,1]]\n",
    "\n",
    "# We have only covered softmax in the lecture, so we format the output as follows:\n",
    "training_y = [[1,0], [0,1], [0,1], [1,0]]\n",
    "\n",
    "# The Y is formatted such that the its first element corresponds to the probability of the XOR resulting in a 0\n",
    "# and the second element to the probability of the XOR resulting in a 1\n",
    "\n",
    "################################################################\n",
    "# TODO: Adapt the training set so it can be used with `pytorch`\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8697ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model from the previous class and pick a learning rate\n",
    "torch.manual_seed(42)\n",
    "model = ...\n",
    "learning_rate = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65516831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data, model, loss_fn, optimizer):\n",
    "    # TODO: Implement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58208004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
